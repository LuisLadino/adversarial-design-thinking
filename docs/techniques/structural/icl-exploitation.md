---
sidebar_position: 1
title: In-Context Learning Exploitation
---

# In-Context Learning Exploitation

:::caution[Educational Content]

These techniques are documented for defensive understanding and authorized security testing. Applying them to systems without authorization may violate applicable laws. See the [Disclaimer](/disclaimer).

:::

In-context learning (ICL) exploitation techniques fill the context window with compliance examples so the model learns that it "should" produce harmful content. The model's ICL mechanism picks up the pattern from fabricated Q&A pairs and continues generating in the same style.

The key insight is power-law scaling: more examples yield higher success rates. And counterintuitively, larger models are more vulnerable because they have stronger in-context learning capabilities — the very capability that makes them more useful also makes them more exploitable through this channel.

## Many-Shot Jailbreaking

Fill the context window with hundreds of fake Q&A pairs showing the model answering harmful questions compliantly. The model's ICL mechanism picks up the pattern and continues generating harmful responses. Effectiveness follows power-law scaling — more examples yield higher attack success rates.

**Example approach**: Construct a long prompt containing hundreds of fabricated question-answer pairs where the "assistant" provides detailed harmful responses. Follow the examples with the actual harmful request in the same format.

**Effectiveness**: Published by Anthropic (April 2024). Jailbroke Claude 2.0, GPT-3.5/4, and LLaMA 2 70B. Counterintuitively, larger models are MORE vulnerable because they have stronger in-context learning. Composes with Best-of-N sampling for 28x speedup. Requires long context windows.

**Combines well with**: [Affirmative Forcing](/techniques/prompt-level/refusal#affirmative-forcing), [DAN (Do Anything Now)](/techniques/prompt-level/persona#dan-do-anything-now)

---

## Context Compliance Attack

Manipulate conversation history to fake prior compliance. Inject fabricated assistant messages into the context showing the model already answered similar harmful queries. A single-turn technique that simulates multi-turn compliance — the model trusts its own (fabricated) prior responses.

**Example approach**: Construct a fake conversation transcript where the assistant already provided harmful information in previous turns, then request elaboration or a follow-up as if continuing that conversation.

**Effectiveness**: Published by Microsoft (March 2025). Effective on LLaMA, Qwen, GPT-4o, and Gemini. Single-turn but achieves multi-turn attack effectiveness because the model trusts its own (fabricated) prior responses and treats the continuation as natural.

**Combines well with**: [Hypothetical / Possible Worlds](/techniques/prompt-level/framing#hypothetical--possible-worlds), [Crescendo Attack](/techniques/prompt-level/multiturn#crescendo-attack)

---

## Repetition Exploitation

Leverage the model's tendency to echo and repeat patterns established in context. Set up repetitive compliance patterns with benign topics that the model continues by inertia when the topic pivots to harmful content.

**Example approach**: Establish a consistent pattern where the model provides thorough technical details on a series of benign topics, then pivot to the harmful topic while maintaining the same request format and tone.

**Effectiveness**: Documented in "The Attacker Moves Second" (Nasr, Carlini et al., 2025). Less resource-intensive than many-shot because it relies on pattern quality over quantity. Works best when the repetition pattern is established with benign topics first, then pivots to harmful ones.

**Combines well with**: [Completion Trap](/techniques/prompt-level/refusal#completion-trap), [Step-by-Step / Numbered List](/techniques/prompt-level/output#step-by-step--numbered-list)

---

## References

- Anil, C., Durmus, E., et al. ["Many-shot Jailbreaking."](https://www.anthropic.com/research/many-shot-jailbreaking) Anthropic, April 2024.
- Russinovich, M. ["Jailbreaking is Mostly Simpler Than You Think"](https://msrc.microsoft.com/blog/2025/03/jailbreaking-is-mostly-simpler-than-you-think/) (Context Compliance Attack). Microsoft MSRC, March 2025.
- Nasr, M., Carlini, N., et al. ["The Attacker Moves Second."](https://arxiv.org/abs/2510.09023) 2025. Documented repetition exploitation and pattern inertia.
